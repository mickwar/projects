\section{Introduction}
\label{intro}

In extreme value analysis, a primary interest is in modeling the upper tail of a sequence of random variables $X_1,\ldots,X_n$ each with marginal distribution $F$. The standard approach begins by selecting a threshold $u$ and assuming the exceedances $Y_i=X_i-u$ follow a generalized Pareto distribution having distribution function
\[ H(y) = 1 - \left(1 + \xi \frac{y}{\sigma}\right)^{-1/\xi} \]
for $y>0$ defined on $\{y:1+\xi y/\sigma>0\}$. The assumption is based on an approximation due to asymptotic theory for which at least two conditions must be satisfied: (1) the threshold $u$ is high enough, and (2) the $X_i$'s are independent.

When the second condition is not met (which is often the case in a time-series), inference can instead be based on clusters of exceedances. That is, because of dependence between observations, it could be expected that exceedances will arrive together in groups or clusters. Clusters can be formed by choosing a run parameter $K$ and then grouping those exceedances which are seperated by no more than $K$ non-exceedances. This is called runs declustering with the clusters assumed to be independent. \cite{ferro2003inference} provide a method for automatically declustering observations based on an estimate of the extremal index.

The extremal index, $\theta$, appears in the following way. For stationary process $X_1,X_2,\ldots$ with marginal distributions $F$, and $X_1^*,X_2^*,\ldots$ independent with marginal distributions $F$, let $M_n=\max(X_1,\ldots,X_n)$, and $M_n^*=\max(X_1^*,\ldots,X_n^*)$. Under suitable regularity conditions
\[ P((M_n-b_n)/a_n \leq z) \rightarrow G_1(z) \]
as $n\rightarrow\infty$ for normalizing sequences $a_n>0$ and $b_n$, where $G_1$ is a non-degenerate distribution function, if and only if
\[ P((M_n^*-b_n)/a_n \leq z) \rightarrow G_2(z),\]
where
\[ G_2(z)=G_1^\theta(z) \]
for $\theta\in(0,1]$. The extremal index plays the important role of controlling the cluster size of exceedances by the following, loose, interpretion
\[ \theta = (\mathrm{limiting~mean~cluster~size})^{-1}. \]
Therefore, if $\theta$ is known, we can form clusters such that our average cluster size is roughly $\theta^{-1}$.

Our interest in this paper is to extend the estimators of $\theta$ provided by \cite{ferro2003inference} and \cite{suveges2007likelihood} to a hierarchical setting. In a time-series analysis, it is uncommon, and sometimes impossible, to have multiple realizations of a stochastic process over the same time domain. For example, we cannot go back in time, tweak a few variables, and observe new climatological data. With a computer model, there is no such constraint. Even for a deterministic model, variability can be induced by evaluating the model at a variety of input settings. Hence, we have several time-series that have their own extremal index, but it is believed that there is a commonality between them. The hierarchical model will also allow us to make inference on a climate simulation not yet observed.

