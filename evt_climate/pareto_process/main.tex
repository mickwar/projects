\documentclass[12pt]{article}

\usepackage{natbib}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{dsfont}
\usepackage[margin=1.00in]{geometry}
\usepackage[font=scriptsize]{caption}
\usepackage{dsfont}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{bm}
\newcommand{\m}[1]{\mathbf{\bm{#1}}}
\newcommand{\R}{I\hspace{-4.4pt}R}
\newcommand{\bc}[1]{\textcolor{blue}{\mathbf{#1}}}
\newcommand{\ind}{\mathds{1}}

\usepackage{enumitem}

\allowdisplaybreaks

\setlength\parindent{0pt}

\begin{document}

\textbf{Definition.} (Constructive approach) A stochastic process $W$ in $C_+(S)$ with constant $\omega_0>0$ is a simple Pareto process if $W(s) = Y V(s)$, for all $s\in S$, for some $Y$ and $V=\{V(s)\}_{s\in S}$ satisfying:
\begin{enumerate}[label=\alph*)]
\item $V\in C_+(S)$ is a stochastic process satisfying $\sup_{s\in S} V(s) = \omega_0$ almost surely, $E[V(s)]>0$ for all $s \in S$,
\item $Y$ is a standard Pareto random variable, $P(Y > y) = y^{-1}, y>1$,
\item $Y$ and $V$ are independent.
\end{enumerate}

See Ferreira and de Haan (2014) for other variants of the definition.
\bigskip

\textbf{Coefficient of asymptotic dependence.} For random variables $X_1$ and $X_2$ having common marginal distribution $F$, let
\[ \chi_{12} = \lim_{z\rightarrow z_+}P(X_1>z|X_2>z) \]
where $z_+$ is the (possibly infinite) right end-point.
\bigskip

For $s_1,s_2\in S$ and $x > \omega_0$, then for $i=1,2$,
\begin{align*}
P(W(s_i)>x) &= P(Y V(s_i) > x) \\
 &= P\left(Y > \frac{x}{V(s_i)}\right) \\
 &= E_{Y,V(s_i)}\left[\ind\left(Y > \frac{x}{V(s_i)}\right)\right] \\
 &= E_{V(s_i)}\left\{E_{Y|V(s_i)}\left[\ind\left(Y > \frac{x}{V(s_i)}\right) \middle| V(s_i) \right]\right\} \\
 &= E_{V(s_i)}\left\{P\left(Y > \frac{x}{V(s_i)} \middle| V(s_i) \right)\right\} \\
 &= E_{V(s_i)}\left\{\frac{V(s_i)}{x}\right\} = \frac{E[V(s_i)]}{x}
\end{align*}
and 
\begin{align*}
P(W(s_1)>x, W(s_2)>x) &= P(Y V(s_1) > x, Y V(s_2) > x) \\
 &= P\left(Y > \frac{x}{V(s_1)}, Y > \frac{x}{V(s_2)}\right) \\
 &= P\left(Y > \frac{x}{V(s_1)}\vee\frac{x}{V(s_2)}\right) \\
 &= P\left(Y > x \left(\frac{1}{V(s_1)}\vee\frac{1}{V(s_2)}\right)\right) \\
 &= P\left(Y > x \left(\frac{1}{V(s_1)\wedge V(s_2)}\right)\right) \\
 &= \frac{E[V(s_1)\wedge V(s_2)]}{x}
\end{align*}
by using arguments similar in the first set of equations. Then for a simple Pareto process at points $s_1,s_2\in S$, we have
\begin{align*}
\chi_{12}^W &= \lim_{x\rightarrow \infty} P(W(s_1) > x | W(s_2) > x) \\
 &= \lim_{x\rightarrow \infty} \frac{xE[V(s_1)\wedge V(s_2)]}{xE[V(s_2)]} \\
 &= \frac{E[V(s_1)\wedge V(s_2)]}{E[V(s_2)]}
\end{align*}
This is from Ferreira and de Haan (2014), but with some clarity (for the dummies of the universe) as to how they got to the expectations.
\bigskip

NOTE: This has some issues since the marginals of $W(s_1)$ and $W(s_2)$ need not be the same. Unless conditioning on them both being above a certain value (i.e. 1 or $\omega_0$), they have different marginals (except the degenerate case when $W(s_1)=W(s_2)$ a.s. where they are both standard paretos. We did not explicitly account for this conditioning in the above calculation.
\bigskip

\textbf{Better $\chi$}. From before, we can find the distribution function for $W_i\equiv W(s_i)$, as
\[ F_{W_i}(w) = 1 - \frac{E(V_i)}{w} \]
where $V_i\equiv V(s_i)$. Using this fact, we can standardize $W_i$ to be uniform and compute $\chi$ in this way
\begin{align*}
\chi_1 &= \lim_{u\rightarrow 1} P(F_{W_1}(W_1) > u | F_{W_2}(W_2) > u) \\
&= \lim_{u\rightarrow 1} P\left(1-\frac{E(V_1)}{W_1} > u \middle| 1-\frac{E(V_2)}{W_2} > u\right) \\
&= \lim_{u\rightarrow 1} P\left(W_1 > \frac{E(V_1)}{1-u} \middle| W_2 > \frac{E(V_2)}{1-u}\right) \\
&= \lim_{u\rightarrow 1} \frac{P\left(W_1 > \frac{E(V_1)}{1-u}, W_2 > \frac{E(V_2)}{1-u}\right)}{P\left(W_2 > \frac{E(V_2)}{1-u}\right)} \\
&= \lim_{u\rightarrow 1} \frac{P\left(YV_1 > \frac{E(V_1)}{1-u}, YV_2 > \frac{E(V_2)}{1-u}\right)}{1-u} \\
&= \lim_{u\rightarrow 1} \frac{1}{1-u}P\left(Y > \frac{E(V_1)}{(1-u)V_1}, Y > \frac{E(V_2)}{(1-u)V_2}\right) \\
&= \lim_{u\rightarrow 1} \frac{1}{1-u}P\left(Y > \frac{E(V_1)}{(1-u)V_1} \vee \frac{E(V_2)}{(1-u)V_2}\right) \\
&= \lim_{u\rightarrow 1} \frac{1}{1-u}P\left(Y > \frac{1}{1-u}\left(\frac{E(V_1)}{V_1} \vee \frac{E(V_2)}{V_2}\right)\right) \\
&= \lim_{u\rightarrow 1} \frac{1}{1-u}(1-u) E\left[\left(\frac{E(V_1)}{V_1} \vee \frac{E(V_2)}{V_2}\right)^{-1}\right] \\
&= E\left(\frac{V_1}{E(V_1)} \wedge \frac{V_2}{E(V_2)}\right) \\
\end{align*}
which is nice and symmetric. Concerns: Are we correctly using the fact that $Y$ is standard Pareto? If we don't explicitly state that $x>1$:
\[ P(Y > x) = \min(1, 1/x) \]

Another concern goes back to solving the distribution function for $W_i$. Since $V_i$ is a random variable, we required conditioning on it using iterated expectations to get the desired result. However, it is possible that $P(V_i == 0) > 0$, and so dividing by $V_i$ may pose an issue. This may be circumvented perhaps by conditioning on $V_i > 0$, but how would this affect the calculation?
\bigskip

For $x>0$,
\begin{align*}
P(W_i > x) &= P(YV_i > x)  \\
 &= P(YV_i > x | V_i = 0)P(V_i = 0) + P(YV_i > x | V_i > 0)P(V_i > 0)  \\
 &= P(0 > x | V_i = 0)P(V_i = 0) + P(YV_i > x | V_i > 0)P(V_i > 0)  \\
 &= P(Y>x/V_i|V_i=0) P(V_i = 0) + P(YV_i > x | V_i > 0)P(V_i > 0)  \\
 &= P(Y>\infty) P(V_i = 0) + P(YV_i > x | V_i > 0)P(V_i > 0)  \\
 &= 0\times P(V_i = 0) + P(YV_i > x | V_i > 0)P(V_i > 0)  \\
 &= P(YV_i > x | V_i > 0)P(V_i > 0)  \\
 &\overset{?}{=} P(V_i>0)\times \frac{E(V_i)}{x}
\end{align*}

It is certainly possible in practice to observe a $V_i=0$. For the joint distribution we may need to condition on $V_1 \wedge V_2 > 0$.

\begin{align*}
\chi_2 &= \lim_{u\rightarrow 1} P(F_{W_1}(W_1) > u | F_{W_2}(W_2) > u) \\
&= \lim_{u\rightarrow 1} P\left(1-\frac{E(V_1)}{W_1} > u \middle| 1-\frac{E(V_2)}{W_2} > u\right) \\
&= \lim_{u\rightarrow 1} P\left(W_1 > \frac{E(V_1)}{1-u} \middle| W_2 > \frac{E(V_2)}{1-u}\right) \\
&= \lim_{u\rightarrow 1} \frac{P\left(W_1 > \frac{E(V_1)}{1-u}, W_2 > \frac{E(V_2)}{1-u}\right)}{P\left(W_2 > \frac{E(V_2)}{1-u}\right)} \\
&= \lim_{u\rightarrow 1} \frac{P\left(YV_1 > \frac{E(V_1)}{1-u}, YV_2 > \frac{E(V_2)}{1-u}\right)}{1-u} \\
&= \lim_{u\rightarrow 1}\Bigg[ \frac{P\left(YV_1 > \frac{E(V_1)}{1-u}, YV_2 > \frac{E(V_2)}{1-u} \middle| V_1 \wedge V_2 = 0\right)P\left(V_1\wedge V_2 = 0\right)}{1-u} \\
&~~~~~+ \frac{P\left(YV_1 > \frac{E(V_1)}{1-u}, YV_2 > \frac{E(V_2)}{1-u} \middle| V_1 \wedge V_2 > 0\right)P\left(V_1\wedge V_2 > 0\right)}{1-u} \Bigg] \\
&= \lim_{u\rightarrow 1}\Bigg[ \frac{0\times P\left(V_1\wedge V_2 = 0\right)}{1-u} \\
&~~~~~+ \frac{P\left(YV_1 > \frac{E(V_1)}{1-u}, YV_2 > \frac{E(V_2)}{1-u} \middle| V_1 \wedge V_2 > 0\right)P\left(V_1\wedge V_2 > 0\right)}{1-u} \Bigg] \\
&= \lim_{u\rightarrow 1}\frac{P\left(YV_1 > \frac{E(V_1)}{1-u}, YV_2 > \frac{E(V_2)}{1-u} \middle| V_1 \wedge V_2 > 0\right)P\left(V_1\wedge V_2 > 0\right)}{1-u}  \\
&= \lim_{u\rightarrow 1} \frac{1}{1-u}P\left(Y > \frac{E(V_1)}{(1-u)V_1}, Y > \frac{E(V_2)}{(1-u)V_2}\middle| V_1 \wedge V_2 > 0 \right) P\left(V_1 \wedge V_2 > 0 \right) \\
&= \lim_{u\rightarrow 1} \frac{1}{1-u}P\left(Y > \frac{E(V_1)}{(1-u)V_1} \vee \frac{E(V_2)}{(1-u)V_2}\middle| V_1 \wedge V_2 > 0 \right)  P\left(V_1 \wedge V_2 > 0 \right)\\
&= \lim_{u\rightarrow 1} \frac{1}{1-u}P\left(Y > \frac{1}{1-u}\left(\frac{E(V_1)}{V_1} \vee \frac{E(V_2)}{V_2}\right)\middle| V_1 \wedge V_2 > 0 \right)  P\left(V_1 \wedge V_2 > 0 \right)\\
&= \lim_{u\rightarrow 1} \frac{1}{1-u}(1-u) E\left[\left(\frac{E(V_1)}{V_1} \vee \frac{E(V_2)}{V_2}\right)^{-1}\middle| V_1 \wedge V_2 > 0 \right]  P\left(V_1 \wedge V_2 > 0 \right)\\
&= E\left(\frac{V_1}{E(V_1)} \wedge \frac{V_2}{E(V_2)}\middle| V_1 \wedge V_2 > 0 \right)  P\left(V_1 \wedge V_2 > 0 \right)\\
\end{align*}

(Note the expectations within the larger expectation (i.e. $E(V_1)$ and $E(V_2)$) are not condition on $V_1 \wedge V_2 > 0$.) When conditioning $V_1 \wedge V_2 > 0$, there was no difference when computing estimates for $\chi_1$ and $\chi_2$. This is perhaps due to the following.
\bigskip

Suppose we have samples $X_1,\ldots,X_n$ having p.d.f. $f(x) = p\delta_0(x) + (1-p)g(x)$ where $g(x)$ is defined on $(0, \infty)$. Then

\begin{align*}
E(X) &= \int_0^\infty x p \delta_0(x) dx + \int_0^\infty x (1-p) g(x) dx \\
&= 0 + (1-p)E_g(X) \\
&= P(X > 0)E_g(X) 
\end{align*}

We could compute the sample mean in two ways:
\[ \bar{X}_1 = \frac{1}{n}\sum_{i=1}^n x_i \]
and
\[ \bar{X}_2 = \left[\frac{1}{n}\sum_{i=1}^n \ind(x_i > 0)\right] \left[\frac{1}{|A|}\sum_{i\in A} x_i\right] \]
where $A=\{x : x > 0\}$ and $|A|$ is the size of $A$. The two multiplicands in $\bar{X}_2$ are estimates for $P(X>0)$ and $E_g(X)$, respectively. Of course, $\bar{X}_1$ and $\bar{X}_2$ are equal, they just use the data differently:
\begin{align*}
\bar{X}_2 &= \left[\frac{1}{n}\sum_{i=1}^n \ind(x_i > 0)\right] \left[\frac{1}{|A|}\sum_{i\in A} x_i\right]  \\
&= \left[\frac{1}{|A|}\sum_{i=1}^n \ind(x_i > 0)\right] \left[\frac{1}{n}\sum_{i\in A} x_i\right]  \\
&= \left[\frac{1}{|A|}\sum_{i=1}^n \ind(x_i > 0)\right] \left[\frac{1}{n}\sum_{i=1}^n x_i\right]  \\
&= \left[1\right] \times \left[\bar{X}_1\right] = \bar{X}_1
\end{align*}
I suspect we have a similar situation with $\chi_1$ and $\chi_2$.

\end{document}
